Jupiter FAQ Bot Architecture Documentation
Overview
The Jupiter FAQ Bot is a Flask-based web application designed to provide instant answers to user queries about Jupiter's financial services. It leverages a combination of natural language processing (NLP), vector search, and large language models (LLMs) to deliver accurate and conversational responses. The system is built to handle FAQ data, preprocess it, and serve answers through a web interface and API endpoints.

Architecture

1. Web Crawler
• Purpose: Extracts FAQs from Jupiter's website to populate the FAQ dataset.
• Details:
o Uses a breadth-first search (BFS) to crawl up to a configurable number of pages (max_pages=150).
o Avoids non-HTML content or restricted URLs (e.g., PDFs, admin pages).
o Employs multiple extraction strategies: FAQ-specific divs, accordion-style FAQs, text patterns, and heading-based extraction.
o Saves FAQs incrementally to faqs_2.json to prevent data loss.
• Implementation: FAQCrawler class in crawler.py, using requests and BeautifulSoup.

2. Data Preprocessing
• Purpose: Cleans and categorizes FAQ data for efficient storage and retrieval.
• Components:
o Cleaning: Removes HTML tags, extra whitespace, and special characters using BeautifulSoup and regex.
o Normalization: Lowercases text, removes stopwords, and lemmatizes using NLTK.
o Categorization: Assigns categories (e.g., KYC, Rewards, Payments) based on keyword matching with fuzzywuzzy.
o Deduplication: Removes duplicate questions using fuzzy matching (fuzz.ratio > 90).
• Implementation: Handled in data.py, which processes raw FAQ data (faqs_2.json) into processed_faq.json and cleaned_faq.json.

3. Embedding Model
• Model: sentence-transformers/all-MiniLM-L6-v2 (default) or ONNX-based ONNXMiniLM_L6_V2 (configurable via USE_ONNX environment variable).
• Purpose: Converts FAQ questions and user queries into dense vector representations for similarity search.
• Details:
o The all-MiniLM-L6-v2 model is a lightweight, high-performance sentence transformer optimized for semantic similarity tasks.
o ONNX support allows for optimized inference on CPU, reducing latency in resource-constrained environments.
o Embeddings are normalized to ensure consistent cosine similarity calculations.
• Implementation: Uses HuggingFaceEmbeddings from the LangChain library or ONNXMiniLM_L6_V2 from ChromaDB, depending on configuration.
4. Vector Database
• Database: ChromaDB
• Purpose: Stores FAQ question embeddings and their associated metadata (answers, categories, source URLs) for efficient similarity search.
• Details:
o Persistent storage is enabled with a directory (chroma_db) to cache embeddings and avoid recomputation.
o FAQs are stored as documents with questions as the primary text and metadata containing answers, categories, and source URLs.
o Similarity search uses cosine similarity to retrieve the most relevant FAQ for a user query.
o Configurable collection name (faq_collection) and persistence directory.
• Implementation: Uses LangChain's Chroma integration for vector storage and retrieval.
5. Large Language Model (LLM)
• Model: Google Gemini 1.5 Flash (gemini-1.5-flash)
• Purpose: Generates conversational responses by rephrasing retrieved FAQ answers.
• Details:
o Configurable parameters: temperature (0.7), max tokens (500).
o Requires a Google API key (GOOGLE_API_KEY) for authentication.
o Uses a prompt template to ensure responses are friendly, concise, and contextually appropriate.
• Implementation: Integrated via LangChain's GoogleGenerativeAI module.
6. Fallback Logic
• Purpose: Handles cases where no relevant FAQ is found or the similarity score is below the threshold.
• Details:
o If the similarity score is below the configured threshold (0.7 by default), the system returns a fallback response:
"I don't have a specific answer for that question. Please check the Jupiter Help Centre or contact our support team for assistance."
o If the ChromaDB is unavailable or no FAQs are loaded, it returns:
"FAQ service is currently unavailable. Please contact support."
o For empty or invalid queries, it returns:
"Please enter a question."
o Errors during processing trigger a generic error response:
"I'm experiencing technical difficulties. Please try again or contact support."
• Implementation: Handled in the FAQBot.answer_question method with appropriate error logging.
7. Web Application
• Framework: Flask
• Endpoints:
o /: Renders the main FAQ interface (index.html).
o /ask: Handles POST requests for user questions (supports both form and JSON inputs).
o /api/ask: JSON-only API endpoint for programmatic access.
o /evaluate: Testing endpoint that runs predefined queries and returns results.
o /health: Health check endpoint to verify service status and FAQ count.
• Error Handling:
o 404: Returns JSON error for invalid endpoints.
o 500: Returns JSON error for internal server issues.
• Frontend: A responsive HTML/CSS/JavaScript interface with a chat-like UI, loading animations, and related question suggestions.




Design Decisions
1. Embedding Model Choice:
o all-MiniLM-L6-v2 was chosen for its balance of performance and efficiency, suitable for FAQ similarity tasks.
o ONNX support was added for environments requiring optimized inference on CPU.
2. Vector Database Selection:
o ChromaDB was selected for its simplicity, persistence, and integration with LangChain, making it ideal for small to medium-sized FAQ datasets.
o Persistent storage ensures embeddings are reused across sessions, reducing startup time.
3. LLM Integration:
o Gemini 1.5 Flash was chosen for its cost-effectiveness and ability to generate conversational responses.
o A prompt template ensures responses are user-friendly and aligned with Jupiter's branding.
4. Fallback Logic:
o A similarity threshold (0.7) ensures only relevant answers are returned, with fallbacks for low-confidence matches.
o Multiple fallback messages provide clear guidance for users when answers are unavailable.
5. Preprocessing and Categorization:
o NLTK and fuzzy matching were used for robust text preprocessing and categorization, ensuring clean and organized FAQ data.
o Deduplication prevents redundant FAQs, improving search efficiency.
6. Web Crawler Design:
o BFS ensures comprehensive coverage of the website while respecting crawl limits.
o Multiple extraction strategies handle diverse FAQ formats, increasing robustness.
o Real-time saving of FAQs prevents data loss during long crawls.
7. Flask Application:
o Flask was chosen for its lightweight nature and ease of integration with Python-based NLP tools.
o Separate /ask and /api/ask endpoints support both web and programmatic access.
o Health and evaluation endpoints facilitate monitoring and testing.

How to Run
Prerequisites:
• Python 3.8+
• Required packages (install via pip install -r requirements.txt):
o flask, langchain, langchain-community, langchain-google-genai
o chromadb, sentence-transformers, onnx (if using ONNX)
o nltk, fuzzywuzzy, pandas, beautifulsoup4, requests
• Google API key (GOOGLE_API_KEY) set in a .env file
• FAQ data file (cleaned_faq.json)

Steps:
1. Clone the Repository:
2. git clone <repository-url>
3. cd <repository-directory>
4. Set Up Environment:
     Create a .env file:
o GOOGLE_API_KEY=<your-google-api-key>
o FAQ_FILE_PATH=cleaned_faq.json
o USE_ONNX=false
o SIMILARITY_THRESHOLD=0.7

     Install dependencies:
o pip install -r requirements.txt
5. Crawl FAQs (Optional):
o Run the crawler to generate faqs_2.json: python crawler.py
o Preprocess the data:  python data.py
6. Run the Flask Application:
  python app.py
o The application will be available at http://localhost:5000.

7. Access the Application:
o Open http://localhost:5000 in a browser for the web interface.
o Use the /api/ask endpoint for programmatic access (e.g., via curl or Postman):
o curl -X POST -H "Content-Type: application/json" -d '{"question":"What are the fees for the Edge card?"}' http://localhost:5000/api/ask 
8. Verify Health:
o Check the service status:
o curl http://localhost:5000/health 
o 

Limitations:
o The all-MiniLM-L6-v2 model may struggle with highly nuanced or context-dependent questions due to its lightweight nature.
o ChromaDB is optimized for small to medium datasets. Large FAQ datasets may require a more scalable solution (e.g., Pinecone or Weaviate).
o Persistence relies on local storage, which may not be ideal for distributed deployments.

